{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16b1a55-9ff7-4d1c-bcf9-610f2284d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f8aafd-bfff-425d-b06c-be354e7e8dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred:  Message: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ReviewCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zoggs</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£38.98</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>1,040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Bear</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£12.99</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bsrpolry</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£19.99</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iCKER</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£19.99</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOULAFASS</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£21.99</td>\n",
       "      <td>4.2 out of 5 stars</td>\n",
       "      <td>1,148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Title  URL   Price              Rating ReviewCount\n",
       "0       Zoggs  N/A  £38.98  4.5 out of 5 stars       1,040\n",
       "1  Harry Bear  N/A  £12.99  5.0 out of 5 stars           1\n",
       "2    Bsrpolry  N/A  £19.99  4.1 out of 5 stars          28\n",
       "3       iCKER  N/A  £19.99  4.4 out of 5 stars         196\n",
       "4   DOULAFASS  N/A  £21.99  4.2 out of 5 stars       1,148"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = webdriver.Safari()\n",
    "driver.get('https://www.amazon.co.uk/')  \n",
    "\n",
    "try:\n",
    "    search_box = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, 'twotabsearchtextbox'))\n",
    "    )\n",
    "    search_box.send_keys('swimwear')\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred: \", e)\n",
    "    driver.quit()\n",
    "\n",
    "product_list = []\n",
    "for page in range(1, 6):  \n",
    "    time.sleep(2) \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    \n",
    "    for product in products:\n",
    "        title = product.h2.text if product.h2 else 'N/A'\n",
    "        url = 'https://www.amazon.co.uk' + product.h2.a['href'] if product.h2 and product.h2.a else 'N/A'\n",
    "        try:\n",
    "            price = product.find('span', 'a-offscreen').text\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        try:\n",
    "            rating = product.find('span', 'a-icon-alt').text\n",
    "        except AttributeError:\n",
    "            rating = 'N/A'\n",
    "        try:\n",
    "            review_count = product.find('span', {'class': 'a-size-base'}).text\n",
    "        except AttributeError:\n",
    "            review_count = 'N/A'\n",
    "        \n",
    "        product_list.append([title, url, price, rating, review_count])\n",
    "    \n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//a[@class=\"s-pagination-item s-pagination-next\"]'))\n",
    "        )\n",
    "        next_button.click()\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "        break  \n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(product_list, columns=['Title', 'URL', 'Price', 'Rating', 'ReviewCount'])\n",
    "df.to_csv('amazon_products.csv', index=False)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da2d04a9-a14f-4226-abd9-5582da7b7ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 144590.01006508333\n",
      "R²: -0.24420340922117711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sales_estimation_model.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('amazon_products.csv')\n",
    "\n",
    "# Clean Data\n",
    "df['Price'] = df['Price'].replace('[£,\\$]', '', regex=True).astype(float)\n",
    "df['Rating'] = df['Rating'].str.extract('(\\d+\\.\\d+)').astype(float)\n",
    "\n",
    "df['ReviewCount'] = df['ReviewCount'].replace('[^0-9]', '', regex=True)\n",
    "df['ReviewCount'] = pd.to_numeric(df['ReviewCount'], errors='coerce').fillna(0).astype(int)\n",
    "np.random.seed(42)\n",
    "df['EstimatedSales'] = np.random.randint(1, 1000, size=len(df))\n",
    "\n",
    "# Select Features and Target Variable\n",
    "features = ['Price', 'Rating', 'ReviewCount']\n",
    "X = df[features]\n",
    "y = df['EstimatedSales']\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R²: {r2}')\n",
    "\n",
    "# Predictions\n",
    "df['SalesPrediction'] = model.predict(X.fillna(0))\n",
    "joblib.dump(model, 'sales_estimation_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d684ed76-2563-4a9c-bc8c-30e10600c662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ReviewCount</th>\n",
       "      <th>EstimatedSales</th>\n",
       "      <th>SalesPrediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zoggs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.98</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1040</td>\n",
       "      <td>103</td>\n",
       "      <td>672.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Bear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.99</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>436</td>\n",
       "      <td>429.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bsrpolry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.99</td>\n",
       "      <td>4.1</td>\n",
       "      <td>28</td>\n",
       "      <td>861</td>\n",
       "      <td>724.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iCKER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.99</td>\n",
       "      <td>4.4</td>\n",
       "      <td>196</td>\n",
       "      <td>271</td>\n",
       "      <td>387.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOULAFASS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.99</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1148</td>\n",
       "      <td>107</td>\n",
       "      <td>195.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.5</td>\n",
       "      <td>30704</td>\n",
       "      <td>72</td>\n",
       "      <td>373.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GRACE KARIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.99</td>\n",
       "      <td>4.7</td>\n",
       "      <td>82</td>\n",
       "      <td>701</td>\n",
       "      <td>532.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>10145</td>\n",
       "      <td>21</td>\n",
       "      <td>168.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.4</td>\n",
       "      <td>14332</td>\n",
       "      <td>615</td>\n",
       "      <td>464.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Joweechy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.99</td>\n",
       "      <td>4.6</td>\n",
       "      <td>605</td>\n",
       "      <td>122</td>\n",
       "      <td>323.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ALISISTER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.99</td>\n",
       "      <td>4.5</td>\n",
       "      <td>35</td>\n",
       "      <td>467</td>\n",
       "      <td>515.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SHEKINI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.99</td>\n",
       "      <td>4.6</td>\n",
       "      <td>50</td>\n",
       "      <td>215</td>\n",
       "      <td>256.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5745</td>\n",
       "      <td>331</td>\n",
       "      <td>462.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SHEKINI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.99</td>\n",
       "      <td>4.2</td>\n",
       "      <td>44</td>\n",
       "      <td>459</td>\n",
       "      <td>464.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Aidotop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1226</td>\n",
       "      <td>88</td>\n",
       "      <td>149.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SHEKINI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.99</td>\n",
       "      <td>4.5</td>\n",
       "      <td>56</td>\n",
       "      <td>373</td>\n",
       "      <td>358.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sixyotie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.99</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3393</td>\n",
       "      <td>100</td>\n",
       "      <td>255.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sukany</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>25</td>\n",
       "      <td>872</td>\n",
       "      <td>735.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.4</td>\n",
       "      <td>28379</td>\n",
       "      <td>664</td>\n",
       "      <td>439.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Beautikini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "      <td>453.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Smismivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>23918</td>\n",
       "      <td>662</td>\n",
       "      <td>587.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.5</td>\n",
       "      <td>30704</td>\n",
       "      <td>309</td>\n",
       "      <td>373.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>heekpek</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.98</td>\n",
       "      <td>4.2</td>\n",
       "      <td>381</td>\n",
       "      <td>770</td>\n",
       "      <td>723.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>YILEEGOO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.88</td>\n",
       "      <td>4.1</td>\n",
       "      <td>205</td>\n",
       "      <td>344</td>\n",
       "      <td>397.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hanna Nikole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.2</td>\n",
       "      <td>66</td>\n",
       "      <td>492</td>\n",
       "      <td>505.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Genfien</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.99</td>\n",
       "      <td>4.1</td>\n",
       "      <td>613</td>\n",
       "      <td>414</td>\n",
       "      <td>448.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>UMIPUBO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.98</td>\n",
       "      <td>4.5</td>\n",
       "      <td>143</td>\n",
       "      <td>806</td>\n",
       "      <td>656.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Genfien</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.99</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1649</td>\n",
       "      <td>386</td>\n",
       "      <td>352.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RXRXCOCO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.99</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>192</td>\n",
       "      <td>408.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>UMIPUBO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.99</td>\n",
       "      <td>4.2</td>\n",
       "      <td>326</td>\n",
       "      <td>956</td>\n",
       "      <td>891.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1493</td>\n",
       "      <td>277</td>\n",
       "      <td>218.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SHEKINI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.99</td>\n",
       "      <td>4.5</td>\n",
       "      <td>162</td>\n",
       "      <td>161</td>\n",
       "      <td>313.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Generic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.99</td>\n",
       "      <td>3.6</td>\n",
       "      <td>31</td>\n",
       "      <td>460</td>\n",
       "      <td>536.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>IBLUELOVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.99</td>\n",
       "      <td>4.2</td>\n",
       "      <td>256</td>\n",
       "      <td>314</td>\n",
       "      <td>562.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>DOULAFASS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.99</td>\n",
       "      <td>4.5</td>\n",
       "      <td>69</td>\n",
       "      <td>22</td>\n",
       "      <td>207.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LULUWINGX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.59</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1347</td>\n",
       "      <td>253</td>\n",
       "      <td>234.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.2</td>\n",
       "      <td>9139</td>\n",
       "      <td>748</td>\n",
       "      <td>259.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AOQUSSQOA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.99</td>\n",
       "      <td>4.1</td>\n",
       "      <td>281</td>\n",
       "      <td>857</td>\n",
       "      <td>811.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sovoyontee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.99</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5455</td>\n",
       "      <td>561</td>\n",
       "      <td>482.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RXRXCOCO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.39</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1015</td>\n",
       "      <td>475</td>\n",
       "      <td>440.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>WIN.MAX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2207</td>\n",
       "      <td>59</td>\n",
       "      <td>152.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>DOULAFASS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.99</td>\n",
       "      <td>4.2</td>\n",
       "      <td>270</td>\n",
       "      <td>511</td>\n",
       "      <td>567.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>GRACE KARIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.99</td>\n",
       "      <td>4.4</td>\n",
       "      <td>182</td>\n",
       "      <td>682</td>\n",
       "      <td>561.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RXRXCOCO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.79</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1988</td>\n",
       "      <td>476</td>\n",
       "      <td>362.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Generic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.89</td>\n",
       "      <td>3.3</td>\n",
       "      <td>140</td>\n",
       "      <td>700</td>\n",
       "      <td>598.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>CharmLeaks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1670</td>\n",
       "      <td>976</td>\n",
       "      <td>353.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Runmeihe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.99</td>\n",
       "      <td>3.5</td>\n",
       "      <td>31</td>\n",
       "      <td>783</td>\n",
       "      <td>545.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.4</td>\n",
       "      <td>28379</td>\n",
       "      <td>190</td>\n",
       "      <td>439.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RITOSTA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>187</td>\n",
       "      <td>958</td>\n",
       "      <td>481.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>CharmLeaks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.99</td>\n",
       "      <td>4.4</td>\n",
       "      <td>79</td>\n",
       "      <td>687</td>\n",
       "      <td>629.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Svanco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.99</td>\n",
       "      <td>4.7</td>\n",
       "      <td>56</td>\n",
       "      <td>958</td>\n",
       "      <td>453.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Joweechy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.79</td>\n",
       "      <td>4.4</td>\n",
       "      <td>252</td>\n",
       "      <td>563</td>\n",
       "      <td>571.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Zoggs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.98</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1040</td>\n",
       "      <td>876</td>\n",
       "      <td>672.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>JASAMBAC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>74</td>\n",
       "      <td>567</td>\n",
       "      <td>549.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>UMIPUBO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.98</td>\n",
       "      <td>4.5</td>\n",
       "      <td>48</td>\n",
       "      <td>244</td>\n",
       "      <td>492.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Yuson Girl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.98</td>\n",
       "      <td>4.5</td>\n",
       "      <td>63</td>\n",
       "      <td>832</td>\n",
       "      <td>637.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>UMIPUBO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>215</td>\n",
       "      <td>505</td>\n",
       "      <td>488.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Dokotoo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.99</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1129</td>\n",
       "      <td>131</td>\n",
       "      <td>255.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Smismivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>23918</td>\n",
       "      <td>485</td>\n",
       "      <td>509.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>CUPSHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.99</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4482</td>\n",
       "      <td>819</td>\n",
       "      <td>605.400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Title  URL  Price  Rating  ReviewCount  EstimatedSales  \\\n",
       "0          Zoggs  NaN  38.98     4.5         1040             103   \n",
       "1     Harry Bear  NaN  12.99     5.0            1             436   \n",
       "2       Bsrpolry  NaN  19.99     4.1           28             861   \n",
       "3          iCKER  NaN  19.99     4.4          196             271   \n",
       "4      DOULAFASS  NaN  21.99     4.2         1148             107   \n",
       "5         CUPSHE  NaN  29.99     4.5        30704              72   \n",
       "6    GRACE KARIN  NaN  26.99     4.7           82             701   \n",
       "7         CUPSHE  NaN  29.99     4.3        10145              21   \n",
       "8         CUPSHE  NaN  29.99     4.4        14332             615   \n",
       "9       Joweechy  NaN  23.99     4.6          605             122   \n",
       "10     ALISISTER  NaN  14.99     4.5           35             467   \n",
       "11       SHEKINI  NaN  27.99     4.6           50             215   \n",
       "12        CUPSHE  NaN  32.99     4.3         5745             331   \n",
       "13       SHEKINI  NaN  27.99     4.2           44             459   \n",
       "14       Aidotop  NaN  24.99     4.3         1226              88   \n",
       "15       SHEKINI  NaN  28.99     4.5           56             373   \n",
       "16      Sixyotie  NaN  24.99     4.1         3393             100   \n",
       "17        Sukany  NaN  24.99     4.3           25             872   \n",
       "18        CUPSHE  NaN  29.99     4.4        28379             664   \n",
       "19    Beautikini  NaN  37.99     NaN            5             131   \n",
       "20      Smismivo  NaN  26.99     4.3        23918             662   \n",
       "21        CUPSHE  NaN  29.99     4.5        30704             309   \n",
       "22       heekpek  NaN  19.98     4.2          381             770   \n",
       "23      YILEEGOO  NaN  19.88     4.1          205             344   \n",
       "24  Hanna Nikole  NaN  29.99     4.2           66             492   \n",
       "25       Genfien  NaN  21.99     4.1          613             414   \n",
       "26       UMIPUBO  NaN  22.98     4.5          143             806   \n",
       "27       Genfien  NaN  19.99     4.4         1649             386   \n",
       "28      RXRXCOCO  NaN  21.99     4.0            3             192   \n",
       "29       UMIPUBO  NaN  24.99     4.2          326             956   \n",
       "30        CUPSHE  NaN  31.99     4.3         1493             277   \n",
       "31       SHEKINI  NaN  27.99     4.5          162             161   \n",
       "32       Generic  NaN  10.99     3.6           31             460   \n",
       "33    IBLUELOVER  NaN  14.99     4.2          256             314   \n",
       "34     DOULAFASS  NaN  26.99     4.5           69              22   \n",
       "35     LULUWINGX  NaN  23.59     4.3         1347             253   \n",
       "36        CUPSHE  NaN  29.99     4.2         9139             748   \n",
       "37     AOQUSSQOA  NaN  23.99     4.1          281             857   \n",
       "38    Sovoyontee  NaN  26.99     3.7         5455             561   \n",
       "39      RXRXCOCO  NaN  20.39     4.1         1015             475   \n",
       "40       WIN.MAX  NaN  29.99     4.3         2207              59   \n",
       "41     DOULAFASS  NaN  21.99     4.2          270             511   \n",
       "42   GRACE KARIN  NaN  26.99     4.4          182             682   \n",
       "43      RXRXCOCO  NaN  23.79     4.3         1988             476   \n",
       "44       Generic  NaN  17.89     3.3          140             700   \n",
       "45    CharmLeaks  NaN  14.99     4.3         1670             976   \n",
       "46      Runmeihe  NaN  12.99     3.5           31             783   \n",
       "47        CUPSHE  NaN  29.99     4.4        28379             190   \n",
       "48       RITOSTA  NaN  17.99     4.3          187             958   \n",
       "49    CharmLeaks  NaN  28.99     4.4           79             687   \n",
       "50        Svanco  NaN  23.99     4.7           56             958   \n",
       "51      Joweechy  NaN  22.79     4.4          252             563   \n",
       "52         Zoggs  NaN  38.98     4.5         1040             876   \n",
       "53      JASAMBAC  NaN  29.99     4.3           74             567   \n",
       "54       UMIPUBO  NaN  16.98     4.5           48             244   \n",
       "55    Yuson Girl  NaN  14.98     4.5           63             832   \n",
       "56       UMIPUBO  NaN  17.99     4.3          215             505   \n",
       "57       Dokotoo  NaN  27.99     4.1         1129             131   \n",
       "58      Smismivo  NaN  27.99     4.3        23918             485   \n",
       "59        CUPSHE  NaN  33.99     4.1         4482             819   \n",
       "\n",
       "    SalesPrediction  \n",
       "0           672.330  \n",
       "1           429.830  \n",
       "2           724.940  \n",
       "3           387.910  \n",
       "4           195.850  \n",
       "5           373.141  \n",
       "6           532.830  \n",
       "7           168.060  \n",
       "8           464.987  \n",
       "9           323.090  \n",
       "10          515.570  \n",
       "11          256.640  \n",
       "12          462.670  \n",
       "13          464.310  \n",
       "14          149.070  \n",
       "15          358.720  \n",
       "16          255.740  \n",
       "17          735.120  \n",
       "18          439.786  \n",
       "19          453.720  \n",
       "20          587.040  \n",
       "21          373.141  \n",
       "22          723.110  \n",
       "23          397.170  \n",
       "24          505.920  \n",
       "25          448.760  \n",
       "26          656.820  \n",
       "27          352.510  \n",
       "28          408.720  \n",
       "29          891.530  \n",
       "30          218.940  \n",
       "31          313.210  \n",
       "32          536.040  \n",
       "33          562.580  \n",
       "34          207.850  \n",
       "35          234.100  \n",
       "36          259.910  \n",
       "37          811.390  \n",
       "38          482.750  \n",
       "39          440.090  \n",
       "40          152.990  \n",
       "41          567.270  \n",
       "42          561.200  \n",
       "43          362.330  \n",
       "44          598.780  \n",
       "45          353.830  \n",
       "46          545.210  \n",
       "47          439.786  \n",
       "48          481.250  \n",
       "49          629.010  \n",
       "50          453.250  \n",
       "51          571.430  \n",
       "52          672.330  \n",
       "53          549.550  \n",
       "54          492.200  \n",
       "55          637.010  \n",
       "56          488.430  \n",
       "57          255.750  \n",
       "58          509.220  \n",
       "59          605.400  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569c7f8f-0875-4fd3-8821-5264b3054875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Mean Squared Error: 120236.94765624336\n",
      "R²: -0.03464423386448434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sales_estimation_model.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('amazon_products.csv')\n",
    "\n",
    "# Clean Data\n",
    "df['Price'] = df['Price'].replace('[£,\\$]', '', regex=True).astype(float)\n",
    "df['Rating'] = df['Rating'].str.extract('(\\d+\\.\\d+)').astype(float)\n",
    "\n",
    "# Clean 'ReviewCount' by extracting only numeric values and setting invalid entries to NaN\n",
    "df['ReviewCount'] = df['ReviewCount'].replace('[^0-9]', '', regex=True)\n",
    "df['ReviewCount'] = pd.to_numeric(df['ReviewCount'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Generate a dummy 'Estimated Sales' column for demonstration\n",
    "np.random.seed(42)\n",
    "df['EstimatedSales'] = np.random.randint(1, 1000, size=len(df))\n",
    "\n",
    "# Select Features and Target Variable\n",
    "features = ['Price', 'Rating', 'ReviewCount']\n",
    "X = df[features]\n",
    "y = df['EstimatedSales']\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R²: {r2}')\n",
    "\n",
    "joblib.dump(best_model, 'sales_estimation_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab2abb91-ba9b-4fbe-acb6-2ba923fce10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred:  Message: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ReviewCount</th>\n",
       "      <th>UnitsSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMAGGIGO</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£22.09</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>1,119</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sam Caan</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£12.99</td>\n",
       "      <td>4.2 out of 5 stars</td>\n",
       "      <td>18</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sixyotie</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£26.98</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>35</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OAMENXI</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£22.94</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>910</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOULAFASS</td>\n",
       "      <td>N/A</td>\n",
       "      <td>£21.99</td>\n",
       "      <td>4.2 out of 5 stars</td>\n",
       "      <td>1,148</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title  URL   Price              Rating ReviewCount UnitsSold\n",
       "0   AMAGGIGO  N/A  £22.09  4.3 out of 5 stars       1,119       N/A\n",
       "1   Sam Caan  N/A  £12.99  4.2 out of 5 stars          18       N/A\n",
       "2   Sixyotie  N/A  £26.98  4.3 out of 5 stars          35       N/A\n",
       "3    OAMENXI  N/A  £22.94  4.3 out of 5 stars         910       N/A\n",
       "4  DOULAFASS  N/A  £21.99  4.2 out of 5 stars       1,148       N/A"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "driver.get('https://www.amazon.co.uk/') \n",
    "\n",
    "try:\n",
    "    search_box = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, 'twotabsearchtextbox'))\n",
    "    )\n",
    "    search_box.send_keys('swimwear')\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred: \", e)\n",
    "    driver.quit()\n",
    "\n",
    "product_list = []\n",
    "\n",
    "# Loop through the first 6 pages\n",
    "for page in range(1, 6):  \n",
    "    time.sleep(2) \n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    \n",
    "    for product in products:\n",
    "        title = product.h2.text if product.h2 else 'N/A'\n",
    "        url = 'https://www.amazon.co.uk' + product.h2.a['href'] if product.h2 and product.h2.a else 'N/A'\n",
    "        try:\n",
    "            price = product.find('span', 'a-offscreen').text\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        try:\n",
    "            rating = product.find('span', 'a-icon-alt').text\n",
    "        except AttributeError:\n",
    "            rating = 'N/A'\n",
    "        try:\n",
    "            review_count = product.find('span', {'class': 'a-size-base'}).text\n",
    "        except AttributeError:\n",
    "            review_count = 'N/A'\n",
    "        \n",
    "        # Check if 'Units Sold' information is available\n",
    "        units_sold_elem = product.find('span', string=lambda text: 'bought in past month' in str(text).lower())\n",
    "        if units_sold_elem:\n",
    "            units_sold = units_sold_elem.text.strip()\n",
    "        else:\n",
    "            units_sold = 'N/A'\n",
    "        \n",
    "        product_list.append([title, url, price, rating, review_count, units_sold])\n",
    "    \n",
    "    # Run to the next page\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//a[@class=\"s-pagination-item s-pagination-next\"]'))\n",
    "        )\n",
    "        next_button.click()\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "        break  # No more pages to navigate\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(product_list, columns=['Title', 'URL', 'Price', 'Rating', 'ReviewCount', 'UnitsSold'])\n",
    "df.to_csv('amazon_products.csv', index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303f32e",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e648a960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25784.0125"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# BSR values for Item 1\n",
    "item1_bsr = [\n",
    "    47018, 46821, 51273, 50996, 55537, 53567, 53677, 52380, 52808, 53395,\n",
    "    30629, 32038, 36611, 45217, 44242, 44554, 50416, 48065, 21292, 34520,\n",
    "    34464, 43719, 48889, 45749, 48348, 52281, 51623, 54253, 53859, 57027,\n",
    "    55836, 55434, 55712, 56839, 56471, 18093, 20396, 33212, 41525, 39927,\n",
    "    43527, 19003, 18707, 32102, 11040, 16416, 13530, 13656, 17660, 4433,\n",
    "    5435, 6372, 1556, 1836, 1551, 2166, 1196, 1684, 1232, 983, 1193, 1047,\n",
    "    1239, 1100, 1256, 1099, 904, 956, 1088, 1242, 1006, 773, 807, 916, 1115,\n",
    "    957, 908, 954, 677, 686\n",
    "]\n",
    "\n",
    "# Calculate average BSR for Item 1\n",
    "avg_bsr_item1 = np.mean(item1_bsr)\n",
    "avg_bsr_item1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f62e1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2573.067415730337"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BSR values for Item 2\n",
    "item2_bsr = [\n",
    "    4169, 4946, 3210, 3003, 3688, 2585, 3030, 2475, 3036, 3152, 3037, 2484,\n",
    "    3132, 3498, 4873, 4985, 3055, 2809, 3166, 2100, 2583, 2923, 3150, 2460,\n",
    "    1953, 2938, 2719, 2862, 3782, 2557, 2380, 3512, 2760, 3247, 2634, 2353,\n",
    "    2541, 2403, 3065, 2766, 3013, 2678, 2789, 2294, 2177, 2306, 1936, 2769,\n",
    "    3265, 2309, 1606, 1999, 2390, 2796, 2383, 2379, 3535, 3296, 2048, 2039,\n",
    "    2258, 2325, 1676, 2720, 2424, 2531, 2627, 2420, 2376, 2650, 1900, 2089,\n",
    "    1802, 1958, 1562, 1765, 2111, 1829, 1825, 1320, 1338, 2310, 1371, 1507,\n",
    "    1577, 1588, 1328, 1466, 2322\n",
    "]\n",
    "\n",
    "# Calculate average BSR for Item 2\n",
    "avg_bsr_item2 = np.mean(item2_bsr)\n",
    "avg_bsr_item2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ad42b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5396.333333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BSR values for Item 3\n",
    "item3_bsr = [\n",
    "    3753, 4164, 4881, 5865, 8664, 5268, 6100, 4687, 3791, 4121, 3931, 5175,\n",
    "    5224, 5794, 5303, 6076, 10001, 5476, 6899, 4628, 5033, 5687, 7552, 6539,\n",
    "    4674, 4298, 5182, 5887, 4762, 6224, 4605, 4280, 5579, 3878, 7281, 6480,\n",
    "    7201, 7268, 8391, 7457, 4784, 4531, 3409, 4640, 4014, 4719, 3274, 4088,\n",
    "    4571, 4697, 4482, 6466, 7212, 7276, 4720, 4375, 4342, 5255, 4182, 4806,\n",
    "    5055, 5818, 7791, 5619, 5641, 5601, 3256, 4338, 5228, 4821, 5748, 3858,\n",
    "    4824, 3480, 3426, 3675, 3617, 4668, 5298, 5896, 7960, 5509, 4786, 6415,\n",
    "    9658, 7485, 6108\n",
    "]\n",
    "\n",
    "# Calculate average BSR for Item 3\n",
    "avg_bsr_item3 = np.mean(item3_bsr)\n",
    "avg_bsr_item3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb62a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.19766548133339004, 1663.002774235962)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define power law function\n",
    "def power_law(x, a, k):\n",
    "    return k * np.power(x, a)\n",
    "\n",
    "# Average BSR values\n",
    "avg_bsr_values = [avg_bsr_item1, avg_bsr_item2, avg_bsr_item3]\n",
    "\n",
    "# Sales values\n",
    "sales_values = [265, 408, 209]\n",
    "\n",
    "# Fit the curve\n",
    "params, _ = curve_fit(power_law, avg_bsr_values, sales_values)\n",
    "\n",
    "# Extracting the parameters\n",
    "a, k = params\n",
    "a, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b90ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.19766548133339004, 1663.002774235962)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_bsr_values = [avg_bsr_item1, avg_bsr_item2, avg_bsr_item3]\n",
    "sales_values = [265, 408, 209]\n",
    "\n",
    "params, _ = curve_fit(power_law, avg_bsr_values, sales_values)\n",
    "\n",
    "a, k = params\n",
    "a, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ca13891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223.31672655249332 352.1798487455544 304.218677673866\n"
     ]
    }
   ],
   "source": [
    "#test power law model\n",
    "EstSales1= k*avg_bsr_item1**a\n",
    "EstSales2= k*avg_bsr_item2**a\n",
    "EstSales3= k*avg_bsr_item3**a\n",
    "print(EstSales1,EstSales2,EstSales3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca54c5",
   "metadata": {},
   "source": [
    "# test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a30a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN and inf values in X_train:\n",
      "day_of_week    0\n",
      "hour           0\n",
      "BSR            0\n",
      "Item_Item1     0\n",
      "Item_Item2     0\n",
      "Item_Item3     0\n",
      "dtype: int64\n",
      "day_of_week    0\n",
      "hour           0\n",
      "BSR            0\n",
      "Item_Item1     0\n",
      "Item_Item2     0\n",
      "Item_Item3     0\n",
      "dtype: int64\n",
      "Checking for NaN and inf values in y_train:\n",
      "0\n",
      "0\n",
      "Checking for excessively large values:\n",
      "day_of_week    0\n",
      "hour           0\n",
      "BSR            0\n",
      "Item_Item1     0\n",
      "Item_Item2     0\n",
      "Item_Item3     0\n",
      "dtype: int64\n",
      "Models trained successfully.\n",
      "Checking for NaN and inf values in X_test:\n",
      "day_of_week    0\n",
      "hour           0\n",
      "BSR            0\n",
      "Item_Item1     0\n",
      "Item_Item2     0\n",
      "Item_Item3     0\n",
      "dtype: int64\n",
      "day_of_week    0\n",
      "hour           0\n",
      "BSR            0\n",
      "Item_Item1     0\n",
      "Item_Item2     0\n",
      "Item_Item3     0\n",
      "dtype: int64\n",
      "Checking for excessively large values in X_test:\n",
      "day_of_week    0\n",
      "hour           0\n",
      "BSR            0\n",
      "Item_Item1     0\n",
      "Item_Item2     0\n",
      "Item_Item3     0\n",
      "dtype: int64\n",
      "Predicted monthly sales for Items 4, 5, 6 using Random Forest: {'Item4': 239.8, 'Item5': 239.8, 'Item6': 239.8}\n",
      "Predicted monthly sales for Items 4, 5, 6 using Gradient Boosting: {'Item4': 240.62359947729155, 'Item5': 240.62359947729152, 'Item6': 240.62359947729158}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\amaz\\chart.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Extract the data for each item\n",
    "items = []\n",
    "total_sales = [265, 408, 209]  # Total sales for items 1 to 3\n",
    "for i in range(0, 12, 2):\n",
    "    item_data = data.iloc[:, i:i+2]\n",
    "    item_data.columns = ['Time', 'BSR']  # Renaming to 'BSR' to reflect its usage\n",
    "    item_data['Item'] = f'Item{(i//2) + 1}'\n",
    "    if i < 6:  # Only for items 1 to 3\n",
    "        item_data['Total Sales'] = total_sales[i//2]\n",
    "    items.append(item_data)\n",
    "\n",
    "# Combine data for Items 1-3 (training data) and Items 4-6 (testing data)\n",
    "train_data = pd.concat(items[:3], ignore_index=True)\n",
    "test_data = pd.concat(items[3:], ignore_index=True)\n",
    "\n",
    "# Convert the Time column to datetime type\n",
    "train_data['Time'] = pd.to_datetime(train_data['Time'], errors='coerce')\n",
    "test_data['Time'] = pd.to_datetime(test_data['Time'], errors='coerce')\n",
    "\n",
    "# Ensure no NaT values in the Time column and sort by Time\n",
    "train_data = train_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "test_data = test_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "\n",
    "# Extract features from the Time column\n",
    "train_data['day_of_week'] = train_data['Time'].dt.dayofweek\n",
    "train_data['hour'] = train_data['Time'].dt.hour\n",
    "test_data['day_of_week'] = test_data['Time'].dt.dayofweek\n",
    "test_data['hour'] = test_data['Time'].dt.hour\n",
    "\n",
    "# Function to fill NaN values with the rolling mean of the past 7 days\n",
    "def fill_with_rolling_mean(df, column_name, time_column, window=7):\n",
    "    df.set_index(time_column, inplace=True)\n",
    "    df[column_name] = df[column_name].fillna(df[column_name].rolling(f'{window}D').mean())\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Apply rolling mean function to the necessary columns in train_data and test_data\n",
    "for column in ['day_of_week', 'hour', 'BSR']:\n",
    "    train_data = fill_with_rolling_mean(train_data, column, 'Time')\n",
    "    test_data = fill_with_rolling_mean(test_data, column, 'Time')\n",
    "\n",
    "# Define the feature columns and the target column\n",
    "feature_cols = ['day_of_week', 'hour', 'Item', 'BSR']\n",
    "X_train = pd.get_dummies(train_data[feature_cols])\n",
    "y_train = train_data['Total Sales']\n",
    "\n",
    "X_test = pd.get_dummies(test_data[feature_cols])\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Check and handle NaN and infinite values\n",
    "print(\"Checking for NaN and inf values in X_train:\")\n",
    "print(X_train.isnull().sum())\n",
    "print(X_train.replace([np.inf, -np.inf], np.nan).isnull().sum())\n",
    "\n",
    "print(\"Checking for NaN and inf values in y_train:\")\n",
    "print(y_train.isnull().sum())\n",
    "print(y_train.replace([np.inf, -np.inf], np.nan).isnull().sum())\n",
    "\n",
    "# Handling NaN and inf values\n",
    "X_train.fillna(X_train.mean(), inplace=True)\n",
    "y_train.fillna(y_train.mean(), inplace=True)  # Only if appropriate based on context\n",
    "\n",
    "# Ensure no entries are too large\n",
    "print(\"Checking for excessively large values:\")\n",
    "print((X_train.abs() > 1e30).sum())\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the models\n",
    "try:\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    print(\"Models trained successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred during model training:\", e)\n",
    "\n",
    "# Additional Preprocessing for X_test (add this section before making predictions)\n",
    "\n",
    "# Check and handle NaN and infinite values in X_test\n",
    "print(\"Checking for NaN and inf values in X_test:\")\n",
    "print(X_test.isnull().sum())\n",
    "print(X_test.replace([np.inf, -np.inf], np.nan).isnull().sum())\n",
    "\n",
    "# Handling NaN and inf values\n",
    "X_test.fillna(X_test.mean(), inplace=True)\n",
    "\n",
    "# Ensure no entries are too large\n",
    "print(\"Checking for excessively large values in X_test:\")\n",
    "print((X_test.abs() > 1e30).sum())   \n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "\n",
    "# Aggregate predictions by item\n",
    "test_data['rf_predictions'] = rf_predictions\n",
    "test_data['gb_predictions'] = gb_predictions\n",
    "\n",
    "rf_item_predictions = test_data.groupby('Item')['rf_predictions'].mean()\n",
    "gb_item_predictions = test_data.groupby('Item')['gb_predictions'].mean()\n",
    "\n",
    "# Print aggregated predictions\n",
    "print(\"Predicted monthly sales for Items 4, 5, 6 using Random Forest:\", rf_item_predictions.to_dict())\n",
    "print(\"Predicted monthly sales for Items 4, 5, 6 using Gradient Boosting:\", gb_item_predictions.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da1f3924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted monthly sales for Items 4, 5, 6 using Random Forest: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'rf_predictions': {0: 269.51, 1: 269.51, 2: 345.13}}\n",
      "Predicted monthly sales for Items 4, 5, 6 using Gradient Boosting: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'gb_predictions': {0: 209.00225771890544, 1: 209.00225771890544, 2: 407.99697200052685}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\amaz\\chart.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Extract the data for each item\n",
    "items = []\n",
    "total_sales = [265, 408, 209]  # Total sales for items 1 to 3\n",
    "for i in range(0, 12, 2):\n",
    "    item_data = data.iloc[:, i:i+2]\n",
    "    item_data.columns = ['Time', 'BSR']  # Renaming to 'BSR' to reflect its usage\n",
    "    item_data['Item'] = f'Item{(i//2) + 1}'\n",
    "    if i < 6:  # Only for items 1 to 3\n",
    "        item_data['Total Sales'] = total_sales[i//2]\n",
    "    items.append(item_data)\n",
    "\n",
    "# Combine data for Items 1-3 (training data) and Items 4-6 (testing data)\n",
    "train_data = pd.concat(items[:3], ignore_index=True)\n",
    "test_data = pd.concat(items[3:], ignore_index=True)\n",
    "\n",
    "# Convert the Time column to datetime type\n",
    "train_data['Time'] = pd.to_datetime(train_data['Time'], errors='coerce')\n",
    "test_data['Time'] = pd.to_datetime(test_data['Time'], errors='coerce')\n",
    "\n",
    "# Ensure no NaT values in the Time column and sort by Time\n",
    "train_data = train_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "test_data = test_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "\n",
    "# Summarize BSR data (e.g., average BSR) for the month for items 1 to 3\n",
    "train_summary = train_data.groupby('Item').agg({'BSR': 'mean'}).reset_index()\n",
    "train_summary['Total Sales'] = total_sales\n",
    "\n",
    "# Prepare the training data for the model\n",
    "X_train = train_summary[['BSR']]\n",
    "y_train = train_summary['Total Sales']\n",
    "\n",
    "# Summarize BSR data for items 4 to 6\n",
    "test_summary = test_data.groupby('Item').agg({'BSR': 'mean'}).reset_index()\n",
    "\n",
    "# Prepare the test data for the model\n",
    "X_test = test_summary[['BSR']]\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the models\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "\n",
    "# Print aggregated predictions\n",
    "test_summary['rf_predictions'] = rf_predictions\n",
    "test_summary['gb_predictions'] = gb_predictions\n",
    "\n",
    "print(\"Predicted monthly sales for Items 4, 5, 6 using Random Forest:\", test_summary[['Item', 'rf_predictions']].to_dict())\n",
    "print(\"Predicted monthly sales for Items 4, 5, 6 using Gradient Boosting:\", test_summary[['Item', 'gb_predictions']].to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14bb9d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted monthly sales for Items 4, 5, 6 using Random Forest: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'rf_predictions': {0: 269.51, 1: 269.51, 2: 349.05}}\n",
      "Predicted monthly sales for Items 4, 5, 6 using Gradient Boosting: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'gb_predictions': {0: 209.00225771890544, 1: 209.00225771890544, 2: 407.99697200052685}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\amaz\\chart.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Extract the data for each item\n",
    "items = []\n",
    "total_sales = [265, 408, 209]  # Total sales for items 1 to 3\n",
    "for i in range(0, 12, 2):\n",
    "    item_data = data.iloc[:, i:i+2]\n",
    "    item_data.columns = ['Time', 'BSR']  # Renaming to 'BSR' to reflect its usage\n",
    "    item_data['Item'] = f'Item{(i//2) + 1}'\n",
    "    if i < 6:  # Only for items 1 to 3\n",
    "        item_data['Total Sales'] = total_sales[i//2]\n",
    "    items.append(item_data)\n",
    "\n",
    "# Combine data for Items 1-3 (training data) and Items 4-6 (testing data)\n",
    "train_data = pd.concat(items[:3], ignore_index=True)\n",
    "test_data = pd.concat(items[3:], ignore_index=True)\n",
    "\n",
    "# Convert the Time column to datetime type\n",
    "train_data['Time'] = pd.to_datetime(train_data['Time'], errors='coerce')\n",
    "test_data['Time'] = pd.to_datetime(test_data['Time'], errors='coerce')\n",
    "\n",
    "# Ensure no NaT values in the Time column and sort by Time\n",
    "train_data = train_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "test_data = test_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "\n",
    "# Summarize BSR data for the month for items 1 to 3\n",
    "train_summary = train_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max']\n",
    "}).reset_index()\n",
    "train_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max']\n",
    "train_summary['Total Sales'] = total_sales\n",
    "\n",
    "# Prepare the training data for the model\n",
    "X_train = train_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max']]\n",
    "y_train = train_summary['Total Sales']\n",
    "\n",
    "# Summarize BSR data for items 4 to 6\n",
    "test_summary = test_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max']\n",
    "}).reset_index()\n",
    "test_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max']\n",
    "\n",
    "# Prepare the test data for the model\n",
    "X_test = test_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max']]\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the models\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "\n",
    "# Print aggregated predictions\n",
    "test_summary['rf_predictions'] = rf_predictions\n",
    "test_summary['gb_predictions'] = gb_predictions\n",
    "\n",
    "print(\"Predicted monthly sales for Items 4, 5, 6 using Random Forest:\", test_summary[['Item', 'rf_predictions']].to_dict())\n",
    "print(\"Predicted monthly sales for Items 4, 5, 6 using Gradient Boosting:\", test_summary[['Item', 'gb_predictions']].to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dfe96a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted monthly sales for Items 4, 5, 6 using Random Forest: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'rf_predictions': {0: 269.51, 1: 263.79, 2: 346.81}}\n",
      "Predicted monthly sales for Items 4, 5, 6 using Gradient Boosting: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'gb_predictions': {0: 209.00225771890544, 1: 209.00225771890544, 2: 407.99697200052685}}\n",
      "BSR values for Items 4, 5, 6 in test data:\n",
      "    Item      BSR_mean      BSR_std  BSR_min  BSR_max  BSR_median    BSR_sum\n",
      "0  Item4   9679.022727  4045.351602   3089.0  24564.0      9121.5   851754.0\n",
      "1  Item5  12627.862069  4410.373201   4975.0  27446.0     12343.0  1098624.0\n",
      "2  Item6   2888.011236   837.470508   1817.0   5730.0      2597.0   257033.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\amaz\\chart.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Extract the data for each item\n",
    "items = []\n",
    "total_sales = [265, 408, 209]  # Total sales for items 1 to 3\n",
    "for i in range(0, 12, 2):\n",
    "    item_data = data.iloc[:, i:i+2]\n",
    "    item_data.columns = ['Time', 'BSR']  # Renaming to 'BSR' to reflect its usage\n",
    "    item_data['Item'] = f'Item{(i//2) + 1}'\n",
    "    if i < 6:  # Only for items 1 to 3\n",
    "        item_data['Total Sales'] = total_sales[i//2]\n",
    "    items.append(item_data)\n",
    "\n",
    "# Combine data for Items 1-3 (training data) and Items 4-6 (testing data)\n",
    "train_data = pd.concat(items[:3], ignore_index=True)\n",
    "test_data = pd.concat(items[3:], ignore_index=True)\n",
    "\n",
    "# Convert the Time column to datetime type\n",
    "train_data['Time'] = pd.to_datetime(train_data['Time'], errors='coerce')\n",
    "test_data['Time'] = pd.to_datetime(test_data['Time'], errors='coerce')\n",
    "\n",
    "# Ensure no NaT values in the Time column and sort by Time\n",
    "train_data = train_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "test_data = test_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "\n",
    "# Summarize BSR data for the month for items 1 to 3\n",
    "train_summary = train_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max', 'median', 'sum']\n",
    "}).reset_index()\n",
    "train_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']\n",
    "train_summary['Total Sales'] = total_sales\n",
    "\n",
    "# Prepare the training data for the model\n",
    "X_train = train_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']]\n",
    "y_train = train_summary['Total Sales']\n",
    "\n",
    "# Summarize BSR data for items 4 to 6\n",
    "test_summary = test_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max', 'median', 'sum']\n",
    "}).reset_index()\n",
    "test_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']\n",
    "\n",
    "# Prepare the test data for the model\n",
    "X_test = test_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']]\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the models\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "\n",
    "# Print aggregated predictions\n",
    "test_summary['rf_predictions'] = rf_predictions\n",
    "test_summary['gb_predictions'] = gb_predictions\n",
    "\n",
    "print(\"Predicted monthly sales for Items 4, 5, 6 using Random Forest:\", test_summary[['Item', 'rf_predictions']].to_dict())\n",
    "print(\"Predicted monthly sales for Items 4, 5, 6 using Gradient Boosting:\", test_summary[['Item', 'gb_predictions']].to_dict())\n",
    "\n",
    "# Debugging - Check the BSR values for items 4 and 5\n",
    "print(\"BSR values for Items 4, 5, 6 in test data:\")\n",
    "print(test_summary[['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cde3b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest predictions for Items 4, 5, 6: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'rf_predictions': {0: 269.51, 1: 263.79, 2: 346.81}}\n",
      "Gradient Boosting predictions for Items 4, 5, 6: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'gb_predictions': {0: 209.00225771890544, 1: 209.00225771890544, 2: 407.99697200052685}}\n",
      "Random Forest feature importances:\n",
      "BSR_median    0.197372\n",
      "BSR_max       0.187359\n",
      "BSR_std       0.175595\n",
      "BSR_sum       0.173843\n",
      "BSR_mean      0.156445\n",
      "BSR_min       0.109386\n",
      "dtype: float64\n",
      "Gradient Boosting feature importances:\n",
      "BSR_mean      0.305407\n",
      "BSR_median    0.259532\n",
      "BSR_sum       0.195886\n",
      "BSR_max       0.135583\n",
      "BSR_std       0.102189\n",
      "BSR_min       0.001404\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\amaz\\chart.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Extract the data for each item\n",
    "items = []\n",
    "total_sales = [265, 408, 209]  # Total sales for items 1 to 3\n",
    "for i in range(0, 12, 2):\n",
    "    item_data = data.iloc[:, i:i+2]\n",
    "    item_data.columns = ['Time', 'BSR']  # Renaming to 'BSR' to reflect its usage\n",
    "    item_data['Item'] = f'Item{(i//2) + 1}'\n",
    "    if i < 6:  # Only for items 1 to 3\n",
    "        item_data['Total Sales'] = total_sales[i//2]\n",
    "    items.append(item_data)\n",
    "\n",
    "# Combine data for Items 1-3 (training data) and Items 4-6 (testing data)\n",
    "train_data = pd.concat(items[:3], ignore_index=True)\n",
    "test_data = pd.concat(items[3:], ignore_index=True)\n",
    "\n",
    "# Convert the Time column to datetime type\n",
    "train_data['Time'] = pd.to_datetime(train_data['Time'], errors='coerce')\n",
    "test_data['Time'] = pd.to_datetime(test_data['Time'], errors='coerce')\n",
    "\n",
    "# Ensure no NaT values in the Time column and sort by Time\n",
    "train_data = train_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "test_data = test_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "\n",
    "# Summarize BSR data for the month for items 1 to 3\n",
    "train_summary = train_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max', 'median', 'sum']\n",
    "}).reset_index()\n",
    "train_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']\n",
    "train_summary['Total Sales'] = total_sales\n",
    "\n",
    "# Prepare the training data for the model\n",
    "X_train = train_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']]\n",
    "y_train = train_summary['Total Sales']\n",
    "\n",
    "# Summarize BSR data for items 4 to 6\n",
    "test_summary = test_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max', 'median', 'sum']\n",
    "}).reset_index()\n",
    "test_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']\n",
    "\n",
    "# Prepare the test data for the model\n",
    "X_test = test_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']]\n",
    "\n",
    "# Manually set the hyperparameters\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=2, random_state=42)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the models\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "\n",
    "# Print predictions\n",
    "test_summary['rf_predictions'] = rf_predictions\n",
    "test_summary['gb_predictions'] = gb_predictions\n",
    "\n",
    "print(\"Random Forest predictions for Items 4, 5, 6:\", test_summary[['Item', 'rf_predictions']].to_dict())\n",
    "print(\"Gradient Boosting predictions for Items 4, 5, 6:\", test_summary[['Item', 'gb_predictions']].to_dict())\n",
    "\n",
    "# Feature importance for RandomForest\n",
    "rf_feature_importances = pd.Series(rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"Random Forest feature importances:\")\n",
    "print(rf_feature_importances)\n",
    "\n",
    "# Feature importance for GradientBoosting\n",
    "gb_feature_importances = pd.Series(gb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"Gradient Boosting feature importances:\")\n",
    "print(gb_feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a972dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest predictions for Items 4, 5, 6: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'rf_predictions': {0: 267.675, 1: 262.67, 2: 353.65}}\n",
      "Gradient Boosting predictions for Items 4, 5, 6: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'gb_predictions': {0: 209.00000005996813, 1: 209.00000005996813, 2: 407.999999919572}}\n",
      "Ridge predictions for Items 4, 5, 6: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'ridge_predictions': {0: 224.22296783468155, 1: 100.91019076024264, 2: 340.96061154654336}}\n",
      "Lasso predictions for Items 4, 5, 6: {'Item': {0: 'Item4', 1: 'Item5', 2: 'Item6'}, 'lasso_predictions': {0: 174.67060224711884, 1: -15.4952578676494, 2: 359.86324258079765}}\n",
      "Random Forest feature importances:\n",
      "BSR_sum       0.204122\n",
      "BSR_median    0.187077\n",
      "BSR_max       0.169609\n",
      "BSR_std       0.168340\n",
      "BSR_mean      0.166042\n",
      "BSR_min       0.104811\n",
      "dtype: float64\n",
      "Gradient Boosting feature importances:\n",
      "BSR_mean      0.305407\n",
      "BSR_median    0.259532\n",
      "BSR_sum       0.195886\n",
      "BSR_max       0.135583\n",
      "BSR_std       0.102189\n",
      "BSR_min       0.001404\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Ignore convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\amaz\\chart.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Extract the data for each item\n",
    "items = []\n",
    "total_sales = [265, 408, 209]  # Total sales for items 1 to 3\n",
    "for i in range(0, 12, 2):\n",
    "    item_data = data.iloc[:, i:i+2]\n",
    "    item_data.columns = ['Time', 'BSR']  # Renaming to 'BSR' to reflect its usage\n",
    "    item_data['Item'] = f'Item{(i//2) + 1}'\n",
    "    if i < 6:  # Only for items 1 to 3\n",
    "        item_data['Total Sales'] = total_sales[i//2]\n",
    "    items.append(item_data)\n",
    "\n",
    "# Combine data for Items 1-3 (training data) and Items 4-6 (testing data)\n",
    "train_data = pd.concat(items[:3], ignore_index=True)\n",
    "test_data = pd.concat(items[3:], ignore_index=True)\n",
    "\n",
    "# Convert the Time column to datetime type\n",
    "train_data['Time'] = pd.to_datetime(train_data['Time'], errors='coerce')\n",
    "test_data['Time'] = pd.to_datetime(test_data['Time'], errors='coerce')\n",
    "\n",
    "# Ensure no NaT values in the Time column and sort by Time\n",
    "train_data = train_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "test_data = test_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "\n",
    "# Summarize BSR data for the month for items 1 to 3\n",
    "train_summary = train_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max', 'median', 'sum']\n",
    "}).reset_index()\n",
    "train_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']\n",
    "train_summary['Total Sales'] = total_sales\n",
    "\n",
    "# Summarize BSR data for items 4 to 6\n",
    "test_summary = test_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max', 'median', 'sum']\n",
    "}).reset_index()\n",
    "test_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']\n",
    "\n",
    "# Prepare the training and test data\n",
    "X_train = train_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']]\n",
    "y_train = train_summary['Total Sales']\n",
    "X_test = test_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=2, random_state=42)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "ridge_model = Ridge(alpha=1.0, solver='svd')  # Changed solver to 'svd'\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "# Train the models\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.predict(X_test_scaled)\n",
    "gb_predictions = gb_model.predict(X_test_scaled)\n",
    "ridge_predictions = ridge_model.predict(X_test_scaled)\n",
    "lasso_predictions = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "# Print predictions\n",
    "test_summary['rf_predictions'] = rf_predictions\n",
    "test_summary['gb_predictions'] = gb_predictions\n",
    "test_summary['ridge_predictions'] = ridge_predictions\n",
    "test_summary['lasso_predictions'] = lasso_predictions\n",
    "\n",
    "print(\"Random Forest predictions for Items 4, 5, 6:\", test_summary[['Item', 'rf_predictions']].to_dict())\n",
    "print(\"Gradient Boosting predictions for Items 4, 5, 6:\", test_summary[['Item', 'gb_predictions']].to_dict())\n",
    "print(\"Ridge predictions for Items 4, 5, 6:\", test_summary[['Item', 'ridge_predictions']].to_dict())\n",
    "print(\"Lasso predictions for Items 4, 5, 6:\", test_summary[['Item', 'lasso_predictions']].to_dict())\n",
    "\n",
    "# Feature importance for RandomForest\n",
    "rf_feature_importances = pd.Series(rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"Random Forest feature importances:\")\n",
    "print(rf_feature_importances)\n",
    "\n",
    "# Feature importance for GradientBoosting\n",
    "gb_feature_importances = pd.Series(gb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"Gradient Boosting feature importances:\")\n",
    "print(gb_feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "176f0c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest predictions for Items 4, 5, 6: {'Item4': 267.675, 'Item5': 262.67, 'Item6': 353.65}\n",
      "RandomForest MAE: 106.56500000000001\n",
      "GradientBoosting predictions for Items 4, 5, 6: {'Item4': 209.00000005996813, 'Item5': 209.00000005996813, 'Item6': 407.999999919572}\n",
      "GradientBoosting MAE: 76.33333334650276\n",
      "Ridge predictions for Items 4, 5, 6: {'Item4': 224.22296783468155, 'Item5': 100.91019076024264, 'Item6': 340.96061154654336}\n",
      "Ridge MAE: 42.39084901612694\n",
      "Lasso predictions for Items 4, 5, 6: {'Item4': 174.67060224711884, 'Item5': -15.4952578676494, 'Item6': 359.86324258079765}\n",
      "Lasso MAE: 55.10087251132353\n",
      "AdaBoost predictions for Items 4, 5, 6: {'Item4': 209.0, 'Item5': 209.0, 'Item6': 408.0}\n",
      "AdaBoost MAE: 76.33333333333333\n",
      "SVR predictions for Items 4, 5, 6: {'Item4': 264.5911384042137, 'Item5': 264.6400008190844, 'Item6': 265.24883412422804}\n",
      "SVR MAE: 135.66076836635668\n",
      "RandomForest feature importances:\n",
      "BSR_sum       0.204122\n",
      "BSR_median    0.187077\n",
      "BSR_max       0.169609\n",
      "BSR_std       0.168340\n",
      "BSR_mean      0.166042\n",
      "BSR_min       0.104811\n",
      "dtype: float64\n",
      "GradientBoosting feature importances:\n",
      "BSR_mean      0.305407\n",
      "BSR_median    0.259532\n",
      "BSR_sum       0.195886\n",
      "BSR_max       0.135583\n",
      "BSR_std       0.102189\n",
      "BSR_min       0.001404\n",
      "dtype: float64\n",
      "AdaBoost feature importances:\n",
      "BSR_sum       0.644147\n",
      "BSR_max       0.298500\n",
      "BSR_std       0.050777\n",
      "BSR_mean      0.000000\n",
      "BSR_min       0.000000\n",
      "BSR_median    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = r'C:\\Users\\Lenovo\\OneDrive\\Desktop\\amaz\\chart.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Extract the data for each item\n",
    "items = []\n",
    "total_sales = [265, 408, 209]  # Total sales for items 1 to 3\n",
    "for i in range(0, 12, 2):\n",
    "    item_data = data.iloc[:, i:i+2]\n",
    "    item_data.columns = ['Time', 'BSR']  # Renaming to 'BSR' to reflect its usage\n",
    "    item_data['Item'] = f'Item{(i//2) + 1}'\n",
    "    if i < 6:  # Only for items 1 to 3\n",
    "        item_data['Total Sales'] = total_sales[i//2]\n",
    "    items.append(item_data)\n",
    "\n",
    "# Combine data for Items 1-3 (training data) and Items 4-6 (testing data)\n",
    "train_data = pd.concat(items[:3], ignore_index=True)\n",
    "test_data = pd.concat(items[3:], ignore_index=True)\n",
    "\n",
    "# Convert the Time column to datetime type\n",
    "train_data['Time'] = pd.to_datetime(train_data['Time'], errors='coerce')\n",
    "test_data['Time'] = pd.to_datetime(test_data['Time'], errors='coerce')\n",
    "\n",
    "# Ensure no NaT values in the Time column and sort by Time\n",
    "train_data = train_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "test_data = test_data.dropna(subset=['Time']).sort_values(by='Time')\n",
    "\n",
    "# Summarize BSR data for the month for items 1 to 3\n",
    "train_summary = train_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max', 'median', 'sum']\n",
    "}).reset_index()\n",
    "train_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']\n",
    "train_summary['Total Sales'] = total_sales\n",
    "\n",
    "# Summarize BSR data for items 4 to 6\n",
    "test_summary = test_data.groupby('Item').agg({\n",
    "    'BSR': ['mean', 'std', 'min', 'max', 'median', 'sum']\n",
    "}).reset_index()\n",
    "test_summary.columns = ['Item', 'BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']\n",
    "\n",
    "# Prepare the training and test data\n",
    "X_train = train_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']]\n",
    "y_train = train_summary['Total Sales']\n",
    "X_test = test_summary[['BSR_mean', 'BSR_std', 'BSR_min', 'BSR_max', 'BSR_median', 'BSR_sum']]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train multiple models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=2, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    'Ridge': Ridge(alpha=1.0, solver='svd'),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "    'SVR': SVR(C=1.0, epsilon=0.2)\n",
    "}\n",
    "\n",
    "# Evaluate the models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    mae = mean_absolute_error([131, 96, 370], predictions)\n",
    "    results[name] = {\n",
    "        'predictions': predictions,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for name, result in results.items():\n",
    "    print(f\"{name} predictions for Items 4, 5, 6:\", dict(zip(['Item4', 'Item5', 'Item6'], result['predictions'])))\n",
    "    print(f\"{name} MAE: {result['mae']}\")\n",
    "\n",
    "# Print feature importances for tree-based models\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importances = pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "        print(f\"{name} feature importances:\")\n",
    "        print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364fadfd",
   "metadata": {},
   "source": [
    "# Item 4: 131,  Item 5:96,   Item 6: 370 \n",
    "based on Helium10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3ad04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
